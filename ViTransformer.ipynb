{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1X9d2dG3a9Re"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "tekmNjsGfot5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BlWVBTygV5i",
        "outputId": "0e3cf900-63d0-4554-e9c4-36a17738418c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.10MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.29MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_classes = 10\n",
        "num_channels = 1\n",
        "img_size = 28\n",
        "patch_size= 7\n",
        "patch_num = (patch_size// img_size)\n",
        "attn_head = 4\n",
        "embed_dim = 64\n",
        "transformer_blocks = 4\n",
        "mlp_nodes = 64"
      ],
      "metadata": {
        "id": "23b5UJsViKrA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataloader.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_data = dataloader.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "2SFZg5-ygbX2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size = patch_size, stride = patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    # going from ([64, 20, 4, 4]) to ([64, 20, 16])\n",
        "    x = x.flatten(2)\n",
        "    # then going from ([64, 20, 16]) to ([64, 16, 20])\n",
        "    x = x.transpose(1,2)\n",
        "    return x"
      ],
      "metadata": {
        "id": "DGR8_f2ii4p4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ln1 = nn.LayerNorm(embed_dim)\n",
        "    self.mhattn = nn.MultiheadAttention(embed_dim, attn_head, batch_first=True)\n",
        "    self.ln2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_nodes),\n",
        "                             nn.GELU(),\n",
        "                             nn.Linear(mlp_nodes, embed_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    res1 = x\n",
        "    x = self.ln1(x)\n",
        "    x = self.mhattn(x, x, x)[0] + res1 # to get the value at 0th index\n",
        "    res2 = x\n",
        "    x = self.ln2(x)\n",
        "    x = self.mlp(x) + res2\n",
        "    return x"
      ],
      "metadata": {
        "id": "Dm-B7ec0no6T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mlphead = nn.Sequential(nn.LayerNorm(embed_dim),\n",
        "                                 nn.Linear(embed_dim, num_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.mlphead(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "bXOhVGD9prE7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbed()\n",
        "    self.cls_tokens = nn.Parameter(torch.randn(1,1, embed_dim))\n",
        "    self.positional_embed = nn.Parameter(torch.randn(1, patch_num + 1, embed_dim))\n",
        "    self.transformer_blocks = nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])\n",
        "    self.mlphead = MLP_head()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    B = x.shape[0]\n",
        "    cls_tokens = self.cls_tokens.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), 1)\n",
        "    x = x + self.positional_embed\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = x[:, 0] # only the CLS token\n",
        "    x = self.mlphead(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RmVuZpleqf1I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "epochs = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VisionTransformer().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "qXtU-iUasvNT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "\n",
        "  model.train()\n",
        "  total_images = 0\n",
        "  correct_images = 0\n",
        "\n",
        "  for images, labels in train_data:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "    total_images += labels.size(0)\n",
        "    correct_images += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"epoch: {epoch+1}/{epochs}, loss: {train_loss/len(train_data):.4f}, accuracy: {100*correct_images/total_images:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CogVYmiLt_vx",
        "outputId": "96b2ae2c-1898-4848-b8ea-ccaa79f8ba4a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/5, loss: 0.7722, accuracy: 73.81%\n",
            "epoch: 2/5, loss: 0.3205, accuracy: 89.98%\n",
            "epoch: 3/5, loss: 0.2315, accuracy: 92.59%\n",
            "epoch: 4/5, loss: 0.1914, accuracy: 93.94%\n",
            "epoch: 5/5, loss: 0.1623, accuracy: 94.85%\n"
          ]
        }
      ]
    }
  ]
}